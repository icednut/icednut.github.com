{"data":{"site":{"siteMetadata":{"title":"Icednut's Blog","author":"Will Lee"}},"markdownRemark":{"id":"4c222807-e13b-51ae-af2b-fa49a8460c6b","excerpt":"Spark IntroZeppelin > Notebook > 입력 패널 첫번쨰 줄에 %를 입력 후 사용할 언어 입력 (%는 인터프리터 지정을 의미)실제 업무에서는 DataFrames를 많이씀. RDD를 할 때는 DataSets를 쓰는게 속도 향상을 볼 수 있음.DataFrames…","html":"<h2>Spark Intro</h2>\n<ul>\n<li>Zeppelin > Notebook > 입력 패널 첫번쨰 줄에 %를 입력 후 사용할 언어 입력 (%는 인터프리터 지정을 의미)</li>\n<li>실제 업무에서는 DataFrames를 많이씀. RDD를 할 때는 DataSets를 쓰는게 속도 향상을 볼 수 있음.</li>\n<li>DataFrames를 쓰면 Python으로 포팅하기도 쉬움. (DataFrames 추천)</li>\n</ul>\n<h2>Spark DataFrames</h2>\n<ul>\n<li>RDD와 DataFrame의 차이점? Catelog optimizer의 유무 (DataFrames에만 있음. RDD는 디시리얼라이제이션 때문에 Heap 사용량이 많아 성능 저하.)</li>\n<li>\n<p>DataFrames는 테이블 형태로 데이터를 한정했기 때문에 SQL에 국한된 로직만 작성할 수 있는 단점이 있다. (DataFrames를 보완하기 위해 Datasets가 나왔음)</p>\n<ul>\n<li>ex)</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\">  dataFrames<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"weight\"</span> <span class=\"token operator\">&lt;</span> <span class=\"token number\">60</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">// $\"weight\"와 같이 컬럼을 지정하여 필터링 할 수 있다.</span>\n  <span class=\"token comment\">// 컬럼($)으로 쓰면 해당 컬럼에 대한 오퍼레이션을 더 추가할 수 있다. (ex: cast)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<ul>\n<li>\n<p>DataFrames를 쓰면 많이 쓰는 메소드:  show, printSchema</p>\n<ul>\n<li>ex)</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">val peopleRDD = spark.sparkContext.makeRDD(&quot;&quot;&quot;{&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;}}&quot;&quot;&quot; :: Nil)\nval people = spark.read.json(peopleRDD)\n\npeople.show()</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span></span></pre></div>\n<ul>\n<li>ex)</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> wikiDF <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>json<span class=\"token punctuation\">(</span><span class=\"token string\">\"/sparklab/dataset/wikiticker-2015-09-12-sampled.json.gz\"</span><span class=\"token punctuation\">)</span>\n\nwikiDF<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nwikiDF<span class=\"token punctuation\">.</span>printSchema<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span></span></pre></div>\n<ul>\n<li>\n<p>DataFrames에서 explain 메소드를 쓰면 RDBMS와 같이 쿼리 수행 계획을 볼 수 있다.</p>\n<ul>\n<li>ex)</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\">wikiDF<span class=\"token punctuation\">.</span>select<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"page\"</span><span class=\"token punctuation\">,</span> $<span class=\"token string\">\"added\"</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>explain<span class=\"token punctuation\">(</span><span class=\"token boolean\">true</span><span class=\"token punctuation\">)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<ul>\n<li>Shuffle Read, Shuffle Write를 염두해두자. (엄청크면 뭔가 문제가 있다는 신호. 조인을 잘못걸경우 커짐. shuffle은 노드간에 데이터를 파티셔닝 및 섞는 것을 의미)</li>\n<li>explain 볼 때 밑에서부터 위로 읽어나가자.</li>\n<li>\n<p>JSON을 RDD로 만든 뒤 DataFrame로 변환해서 출력해보면 알아서 컬럼 타입을 지정해준다.</p>\n<ul>\n<li>ex)</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> cityRDD <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>sparkContext<span class=\"token punctuation\">.</span>makeRDD<span class=\"token punctuation\">(</span>\n  <span class=\"token string\">\"\"\"{\"cityName\":\"Seoul\",\"countryName\":\"Republic of Korea\",\"gdp\":1321200}\"\"\"</span> <span class=\"token operator\">:</span><span class=\"token operator\">:</span>\n  <span class=\"token string\">\"\"\"{\"cityName\":\"Tokyo\",\"countryName\":\"Japan\",\"gdp\":4412600}\"\"\"</span> <span class=\"token operator\">:</span><span class=\"token operator\">:</span>\n  <span class=\"token string\">\"\"\"{\"cityName\":\"Moscow\",\"countryName\":\"Russia\",\"gdp\":113240}\"\"\"</span> <span class=\"token operator\">:</span><span class=\"token operator\">:</span>\n  <span class=\"token string\">\"\"\"{\"cityName\":\"London\",\"countryName\":\"United Kingdom\",\"gdp\":2760960}\"\"\"</span> <span class=\"token operator\">:</span><span class=\"token operator\">:</span> Nil<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> cityDF <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>json<span class=\"token punctuation\">(</span>cityRDD<span class=\"token punctuation\">)</span>\n\ncityDF<span class=\"token punctuation\">.</span>printSchema<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\ncityDF<span class=\"token punctuation\">.</span>show</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<ul>\n<li>agg는 org.apache.spark.sql.functions를 참조</li>\n<li>DataFrame는 압축된 데이터 바이너리인 텅스텐으로 메모리에 저장됨</li>\n<li>\n<p>z는 제플린 컨텍스트라고 해서 제플린 관련 설정을 바꿀 때 사용</p>\n<ul>\n<li>ex)</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> shufflePartitions <span class=\"token operator\">=</span> z<span class=\"token punctuation\">.</span>input<span class=\"token punctuation\">(</span><span class=\"token string\">\"spark.sql.shuffle.partition\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"200\"</span><span class=\"token punctuation\">)</span>\n                            <span class=\"token punctuation\">.</span>asInstanceOf<span class=\"token punctuation\">[</span><span class=\"token builtin\">String</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>toInt</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<ul>\n<li>\n<p>일반적인 경우는 파티션의 크기를 10MB로 잡자.</p>\n<ul>\n<li>CPU가 많이 먹으면 파티션의 크기를 조정해야됨</li>\n</ul>\n</li>\n<li>\n<p>parquet로 쓰면 컬럼 기반으로 데이터 구조가 잡혀있기 때문에 spark에서 읽을 때 오버헤드가 줄어든다.</p>\n<ul>\n<li>parquet를 쓰면 SQL From 절에 바로 써서 조회할 수 있다.</li>\n<li>parquet를 쓰면 컬럼 기반으로 조회(column pruning)를 해서 분석 프로세스를 더 빨리 진행할 수 있다. (FileScan step에서 이미 필터링을 진행)</li>\n<li>ex)</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> sqlDF <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">(</span><span class=\"token string\">\"SELECT countryName, cityName, sum(gdp) FROM parquet.`/sparklab/tmp-output/wiki_gdp` WHERE gdp > 1321200 GROUP BY countryName, cityName\"</span><span class=\"token punctuation\">)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<h2>UDF (User Defined Function)</h2>\n<ul>\n<li>\n<p>사용자가 만든 함수. 조인을 할 때 udf를 잘못 쓰면 옵티마이저가 안먹힐 수도 있다.</p>\n<ul>\n<li>ex)</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>functions</span><span class=\"token punctuation\">.</span>_\n\n<span class=\"token keyword\">val</span> coder<span class=\"token operator\">:</span> <span class=\"token punctuation\">(</span><span class=\"token builtin\">Int</span> <span class=\"token keyword\">=></span> <span class=\"token builtin\">String</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>added<span class=\"token operator\">:</span> <span class=\"token builtin\">Int</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">=></span> <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>added <span class=\"token operator\">></span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span> <span class=\"token string\">\"frequent\"</span> <span class=\"token keyword\">else</span> <span class=\"token string\">\"rare\"</span>\n<span class=\"token keyword\">val</span> sqlfunc <span class=\"token operator\">=</span> udf<span class=\"token punctuation\">(</span>coder<span class=\"token punctuation\">)</span>\n\nwikiDF<span class=\"token punctuation\">.</span>withColumn<span class=\"token punctuation\">(</span><span class=\"token string\">\"Frequency\"</span><span class=\"token punctuation\">,</span> sqlfunc<span class=\"token punctuation\">(</span>col<span class=\"token punctuation\">(</span><span class=\"token string\">\"added\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>select<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"page\"</span><span class=\"token punctuation\">,</span> $<span class=\"token string\">\"added\"</span><span class=\"token punctuation\">,</span> $<span class=\"token string\">\"frequency\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<ul>\n<li>\n<p>SQL로 만든것을 Spark DataFrame 메소드로 바꿔보기 연습</p>\n<ul>\n<li>ex)</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\">wikiDF\n  <span class=\"token punctuation\">.</span>select<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"countryName\"</span><span class=\"token punctuation\">,</span> $<span class=\"token string\">\"cityName\"</span><span class=\"token punctuation\">,</span> $<span class=\"token string\">\"comment\"</span><span class=\"token punctuation\">,</span> $<span class=\"token string\">\"added\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>groupBy<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"countryName\"</span><span class=\"token punctuation\">,</span> $<span class=\"token string\">\"cityName\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span>avg<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"added\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>alias<span class=\"token punctuation\">(</span><span class=\"token string\">\"avg_added\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> max<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"added\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>alias<span class=\"token punctuation\">(</span><span class=\"token string\">\"max_added\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>orderBy<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"avg_added\"</span><span class=\"token punctuation\">.</span>desc<span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>limit<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>show\n<span class=\"token comment\">// select 컬럼에 내가 보고자 하는 컬럼을 추가해도 groupBy에 지정 안한 컬럼은 결과로 나오지 않음</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<h2>Join</h2>\n<h2>Pivoting</h2>\n<ul>\n<li>데이터를 Row로 쫙 늘리는 것?</li>\n</ul>\n<h2>Wikipedia 연습</h2>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\">pageCountsDF<span class=\"token punctuation\">.</span>withColumn<span class=\"token punctuation\">(</span><span class=\"token string\">\"masked_project\"</span><span class=\"token punctuation\">,</span> regexp_replace<span class=\"token punctuation\">(</span>regexp_replace<span class=\"token punctuation\">(</span>regexp_replace<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"project\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"[0-9]+\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"9\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"[A-Z]+\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"A\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"[a-z]+\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"a\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"masked_project\"</span> <span class=\"token operator\">==</span><span class=\"token operator\">=</span> <span class=\"token string\">\"a-a\"</span> or $<span class=\"token string\">\"masked_project\"</span> <span class=\"token operator\">==</span><span class=\"token operator\">=</span> <span class=\"token string\">\"a-a-a\"</span> or $<span class=\"token string\">\"masked_project\"</span> <span class=\"token operator\">==</span><span class=\"token operator\">=</span> <span class=\"token string\">\"a-a-a.a\"</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">// \"masked_project = 'a-a' or masked_project = 'a-a-a'\" 이렇게도 쓸 수 있음</span>\n  <span class=\"token punctuation\">.</span>groupBy<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"masked_project\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>count\n  <span class=\"token punctuation\">.</span>orderBy<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"count\"</span><span class=\"token punctuation\">.</span>desc<span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<h2>Wikipedia 실습</h2>\n<ul>\n<li>\n<p>spark.catalog.listTables().show(false)</p>\n<ul>\n<li>메소드의 파라미터를 false로 주면 안짤리고 다 나옴</li>\n</ul>\n</li>\n</ul>\n<h2>SKT에서 Spark 활용 사례</h2>\n<ul>\n<li>Flume, Kafka로 데이터를 메세지큐에 담음 -> Batch 처리 or Streaming 처리에 Spark 사용 > 저장소에 저장 (Cache, HDFS)</li>\n<li>카프카는 최소 3대 이상으로 구축되어야 함.</li>\n<li>카프카도 토픽에 데이터가 너무 많이 들어오면 repartition을 진행해야 한다.</li>\n<li>Streaming에서는 Transaction 처리가 힘듬.</li>\n<li>Data를 Write하는 것도 고려해야됨. > 대량으로 write하기 위해서는 MQ를 중간에 사용하여 저장 처리 > 그러나 중복이 발생할 수 있다?</li>\n<li>Catalog API가 뭘까?</li>\n<li>DataSet의 장점: Strong Type (Untyped API, Type API 모두 제공함)</li>\n<li>Shared Variable 사용 방법: Accumulator, Broadcast Variables</li>\n<li>Whole Stage CodeGen (실행 시점에 필요한 데이터만 필터링해서 데이터를 올림, Tungsten vetorization)</li>\n</ul>\n<h2>Structured Streaming</h2>\n<ul>\n<li>Streaming 처리를 할 때 unbounded table 형태로 데이터 전체를 사용할 수 있음 (structured stream outputMode: Append, Complete, Update 때문)</li>\n<li>10분 단위로 트리거링해서 Result Table 생성할 수 있음</li>\n<li>Structured Stream은 아직 알파버전</li>\n</ul>","frontmatter":{"title":"스파크 강의 노트 Day 3","date":"2017-05-29 09:01:47","tags":["Spark"]}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/20170529-spark-lecture-day03/","previous":{"fields":{"slug":"/20170526-spark-lecture-day02/"},"frontmatter":{"title":"스파크 강의 노트 Day 2"}},"next":{"fields":{"slug":"/20170530-spark-lecture-day04/"},"frontmatter":{"title":"스파크 강의 노트 Day 4"}}}}