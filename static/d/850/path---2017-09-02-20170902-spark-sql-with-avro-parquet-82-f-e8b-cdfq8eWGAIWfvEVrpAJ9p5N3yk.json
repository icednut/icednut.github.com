{"data":{"site":{"siteMetadata":{"title":"Icednut's Notes","author":"Will Lee","siteUrl":"https://icednut.github.io"}},"markdownRemark":{"id":"8631f49c-4254-5937-9a5a-2d61dee1573d","excerpt":"준비Scala & Spark SQL에서 avro, parquet 파일을 읽고 쓰는 것은 어떻게 하는지 그리고 간단한 예제를 통해 실습한 내용을 정리한다. (avro와 parquet에 대한 설명은 여기서는 생략)\n먼저 아래 내용들을 통해 실습 환경을 셋팅하자.CDH 설치Google…","html":"<h2>준비</h2>\n<p>Scala &#x26; Spark SQL에서 avro, parquet 파일을 읽고 쓰는 것은 어떻게 하는지 그리고 간단한 예제를 통해 실습한 내용을 정리한다. (avro와 parquet에 대한 설명은 여기서는 생략)\n먼저 아래 내용들을 통해 실습 환경을 셋팅하자.</p>\n<ol>\n<li>\n<p>CDH 설치</p>\n</li>\n<li>\n<p>Google에서 Cloudera CDH 검색</p>\n</li>\n<li>\n<p>CDH > quickstart Virtual Box 버전을 다운로드</p>\n</li>\n<li>\n<p>압축 해제 후 Virtual Box에서 실행</p>\n</li>\n<li>\n<p>sqoop을 통한 avro 파일로 hdfs에 import 작업 진행</p>\n</li>\n<li>\n<p>hdfs의 적당한 곳에 디렉토리를 생성한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">$ hdfs dfs -mkdir /user/cloudera/test_avro_warehouse</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n</li>\n<li>\n<p>sqoop을 통해 MySQL 데이터를 HDFS로 import 한다. (파일 포맷은 avro. 압축 형태는 snappy)</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">$ sqoop import-all-tables \\\n-m 1 \\\n--connect jdbc:mysql://quickstart:3306/retail_db \\\n--username=retail_dba \\\n--password=cloudera \\\n--as-avrodatafile \\\n--compression-codec=snappy \\\n--warehouse-dir=/user/cloudera/test_avro_warehouse</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>\n<p>이번에는 parquet 파일로 hdfs에 import 진행</p>\n</li>\n<li>\n<p>sqoop을 통해 MySQL 데이터를 HDFS로 import 한다. (파일 포맷은 parquet. 압축 형태는 snappy)</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">$ sqoop import-all-tables \\\n-m 1 \\\n--connect jdbc:mysql://quickstart:3306/retail_db \\\n--username=retail_dba \\\n--password=cloudera \\\n--as-parquetfile \\\n--compression-codec=snappy \\\n--warehouse-dir=/user/cloudera/test_avro_warehouse</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n</li>\n<li>\n<p>spark-shell 실행</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-text line-numbers\"><code class=\"language-text\">$ spark-shell</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n</li>\n</ol>\n<h2>Avro</h2>\n<p>Spark에서 avro 파일을 읽고 쓰려면 avro 관련 라이브러리를 import 해야 된다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">com<span class=\"token punctuation\">.</span>databricks<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>avro</span><span class=\"token punctuation\">.</span>_</code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span></span></pre></div>\n<p>그 다음 sqlContext.read.avro(”…“) 혹은 sqlContext().read.format(“com.databricsk.spark.avro”).load(”…“)을 통해 파일을 읽는다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">com<span class=\"token punctuation\">.</span>databricks<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>avro</span><span class=\"token punctuation\">.</span>_\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> sqlContext<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>avro<span class=\"token punctuation\">(</span><span class=\"token string\">\"input dir\"</span><span class=\"token punctuation\">)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">com<span class=\"token punctuation\">.</span>databricks<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>avro</span><span class=\"token punctuation\">.</span>_\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> sqlContext<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token string\">\"com.databricks.spark.avro\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token string\">\"input dir\"</span><span class=\"token punctuation\">)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span></span></pre></div>\n<h5>연습1. <code class=\"language-text\">hdfs://quickstart/user/cloudera/test_avro_warehouse/orders</code>에서 order<em>status가 <code class=\"language-text\">COMPLETE</code> 면서 customer</em>id 별로 주문을 몇건씩 했는지 살펴본 뒤 결과는 parquet로 저장 해보자. (parquet 압축은 gzip)</h5>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">com<span class=\"token punctuation\">.</span>databricks<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>avro</span><span class=\"token punctuation\">.</span>_\n<span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql</span><span class=\"token punctuation\">.</span>_\n  \n<span class=\"token keyword\">val</span> ordersDf <span class=\"token operator\">=</span> sqlContext<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>avro<span class=\"token punctuation\">(</span><span class=\"token string\">\"/user/cloudera/test_avro_warehouse/orders\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> countRdd <span class=\"token operator\">=</span> ordersDf<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span><span class=\"token string\">\"order_status = 'COMPLETE'\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>map<span class=\"token punctuation\">(</span>row <span class=\"token keyword\">=></span> <span class=\"token punctuation\">(</span>row<span class=\"token punctuation\">.</span>getAs<span class=\"token punctuation\">[</span>Integer<span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"order_customer_id\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>reduceByKey<span class=\"token punctuation\">(</span>_ <span class=\"token operator\">+</span> _<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>map<span class=\"token punctuation\">(</span>pair <span class=\"token keyword\">=></span> Row<span class=\"token punctuation\">(</span>pair<span class=\"token punctuation\">.</span>_1<span class=\"token punctuation\">,</span> pair<span class=\"token punctuation\">.</span>_2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  \nsqlContext<span class=\"token punctuation\">.</span>setConf<span class=\"token punctuation\">(</span><span class=\"token string\">\"spark.sql.parquet.compression.codec\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"gzip\"</span><span class=\"token punctuation\">)</span>\n  \n<span class=\"token keyword\">val</span> schema <span class=\"token operator\">=</span> StructType<span class=\"token punctuation\">(</span>\n  StructField<span class=\"token punctuation\">(</span><span class=\"token string\">\"customer_id\"</span><span class=\"token punctuation\">,</span> IntegerType<span class=\"token punctuation\">,</span> <span class=\"token boolean\">false</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">:</span><span class=\"token operator\">:</span>\n  StructField<span class=\"token punctuation\">(</span><span class=\"token string\">\"count\"</span><span class=\"token punctuation\">,</span> IntegerType<span class=\"token punctuation\">,</span> <span class=\"token boolean\">false</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">:</span><span class=\"token operator\">:</span> Nil\n<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> countDf <span class=\"token operator\">=</span> sqlContext<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>countRdd<span class=\"token punctuation\">,</span> schema<span class=\"token punctuation\">)</span>\ncountDf<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">.</span>parquet<span class=\"token punctuation\">(</span><span class=\"token string\">\"/user/cloudera/test_parquet_warehouse/orders_count\"</span><span class=\"token punctuation\">)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<h5>연습2. json 파일을 읽어서 HDFS에 avro 파일로 저장해보자. (with snappy compression)</h5>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">com<span class=\"token punctuation\">.</span>databricks<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>avro</span><span class=\"token punctuation\">.</span>_\n  \n<span class=\"token keyword\">val</span> personJsonDf <span class=\"token operator\">=</span> sqlContext<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>json<span class=\"token punctuation\">(</span><span class=\"token string\">\"/user/cloudera/test_json_warehouse\"</span><span class=\"token punctuation\">)</span>\n  \nsqlContext<span class=\"token punctuation\">.</span>setConf<span class=\"token punctuation\">(</span><span class=\"token string\">\"spark.sql.avro.compression.codec\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"snappy\"</span><span class=\"token punctuation\">)</span>\npersonJsonDf<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">.</span>avro<span class=\"token punctuation\">(</span><span class=\"token string\">\"/user/cloudera/test_avro_warehouse/person\"</span><span class=\"token punctuation\">)</span>\n  \n<span class=\"token keyword\">val</span> personAvroDf <span class=\"token operator\">=</span> sqlContext<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>avro<span class=\"token punctuation\">(</span><span class=\"token string\">\"/user/cloudera/test_avro_warehouse/person\"</span><span class=\"token punctuation\">)</span>\npersonAvroDf<span class=\"token punctuation\">.</span>printSchema<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\npersonAvroDf<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<h2>Parquet</h2>\n<p>Parquet 파일을 다룰 때는 따로 import 해줘야할 라이브러리가 없다. 그냥 <code class=\"language-text\">sqlContext.read.parquet(&quot;inpurt file&quot;)</code>을 통해 parquet 파일을 읽으면 된다.</p>\n<h5>연습3. <code class=\"language-text\">hdfs://quickstart/user/cloudera/test_parquet_warehouse/orders</code>을 읽어서 HDFS에 avro 파일로 저장해보자.</h5>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre style=\"counter-reset: linenumber NaN\" class=\"language-scala line-numbers\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">com<span class=\"token punctuation\">.</span>databricks<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>avro</span><span class=\"token punctuation\">.</span>_\n  \n<span class=\"token keyword\">val</span> ordersCountDf <span class=\"token operator\">=</span> sqlContext<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">.</span>parquet<span class=\"token punctuation\">(</span><span class=\"token string\">\"/user/cloudera/test_parquet_warehouse/orders_count\"</span><span class=\"token punctuation\">)</span>\n  \nsqlContext<span class=\"token punctuation\">.</span>setConf<span class=\"token punctuation\">(</span><span class=\"token string\">\"spark.sql.avro.compression.codec\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"snappy\"</span><span class=\"token punctuation\">)</span>\nordersCountDf<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">.</span>avro<span class=\"token punctuation\">(</span><span class=\"token string\">\"/user/cloudera/test_avro_warehouse/orders_count\"</span><span class=\"token punctuation\">)</span></code><span aria-hidden=\"true\" class=\"line-numbers-rows\" style=\"white-space: normal; width: auto; left: 0;\"><span></span><span></span><span></span><span></span><span></span><span></span></span></pre></div>\n<h2>참고 자료</h2>\n<ul>\n<li><a href=\"https://www.cloudera.com/more/training/certification/cca-spark.html\">https://www.cloudera.com/more/training/certification/cca-spark.html</a></li>\n<li><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_avro_usage.html#concept_okv_lwy_pv\">https://www.cloudera.com/documentation/enterprise/latest/topics/cdh<em>ig</em>avro<em>usage.html#concept</em>okv<em>lwy</em>pv</a></li>\n<li><a href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/spark_avro.html\">https://www.cloudera.com/documentation/enterprise/latest/topics/spark_avro.html</a></li>\n</ul>","frontmatter":{"title":"Spark SQL을 이용하여 avro 파일과 parquet 파일 다루기","date":"2017-09-02 15:38:55","tags":["spark","scala","avro","parquet"]}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/2017/09/02/20170902-spark-sql-with-avro-parquet/","previous":{"fields":{"slug":"/2017/08/30/20170830-oracle-code-seoul/"},"frontmatter":{"title":"Oracle Code Seoul"}},"next":{"fields":{"slug":"/2017/09/04/20170904-java-web-develop-with-spring/"},"frontmatter":{"title":"Java Web 개발 살펴보기 (Model 1 부터 Spring Web 까지)"}}}}